{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abisubramanya27/ChAII-docker/blob/master/ChAII_COPY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTwSEAbMVIBn"
   },
   "source": [
    "Env Setup (Check out https://towardsdatascience.com/conda-google-colab-75f7c867a522 for detailed instructions)\n",
    "BEFORE STARTING, SET RUNTIME_TYPE TO GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVtit-4yq6rg"
   },
   "source": [
    "# ChAII starter notebook\n",
    "\n",
    "This is a starter notebook for running a baseline mBERT model on the task. This is a standalone notebook which will allow you to do the following:\n",
    "1. Train an mBERT model on the ChAII data (utilizing Colab GPUs), \n",
    "2. Get dev evaluation numbers, \n",
    "3. Generate a submission for the Kaggle leaderboard with the appropriate format.\n",
    "\n",
    "This notebook uses the [Xtreme](https://github.com/google-research/xtreme) codebase for training QA-finetuned models. Feel free to run your own scripts/variants locally, experiment with other models and pipelines, not necessarily limited to Xtreme. Here are some caveats of this method:\n",
    "1. Since runtimes (GPU/TPU) are re-allocated when the notebook is idle, Anaconda and dependencies installations need to be rerun everytime the notebook has to be reconnected.\n",
    "2. When runtime is disconnected, you lose all the files you have stored. Therefore you need to mount your Google Drive and store the relevant code and data there. Upon reconnection you can simply remount to access the data.\n",
    "\n",
    "Given the above conditions, we encourage you to have local installations (of Anaconda) and clones of the Xtreme codebase (with some changes mentioned below), use this notebook for training with GPU (and inference), and conduct evaluations locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W29gv5wKxB4v"
   },
   "source": [
    "## Environment + Xtreme Setup\n",
    "\n",
    "These steps set up Anaconda (Miniconda) on Colab, and set up the Xtreme Github codebase. \n",
    "\n",
    "**BEFORE YOU BEGIN:** Ensure you have set runtime as GPU before running. These set of cells have to rerun everytime you disconnect/change runtime.\n",
    "\n",
    "Colab navigation: With Colab, you can access your folders and files with the upper-left icon. Your files will be stored in ```/content/```. \n",
    "\n",
    "Useful links:\n",
    "1. Mounting your Google Drive, Cloud Storage: [link](https://colab.sandbox.google.com/notebooks/io.ipynb#scrollTo=eikfzi8ZT_rW)\n",
    "2. Setting up Miniconda on Colab: [link](https://towardsdatascience.com/conda-google-colab-75f7c867a522)\n",
    "3. Xtreme Github Repo: [link](https://github.com/google-research/xtreme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the ChAII docker image\n",
    "\n",
    "You can directly skip to the data processing and training cells in this notebook if you already have the ChAII docker image. You won't need to modify script nor setup the conda environment. All instrcutions pertaining to that have already been taken care of in the docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-i_Rxpy4paf"
   },
   "source": [
    "### Anaconda (Miniconda) Setup\n",
    "\n",
    "See [this link](https://towardsdatascience.com/conda-google-colab-75f7c867a522) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Zqk1MjtVndG",
    "outputId": "db2c31d4-283b-4a2c-e3db-7596e5c8b541"
   },
   "outputs": [],
   "source": [
    "# Verify PYTHONPATH is blank to avoid problems later\n",
    "!echo $PYTHONPATH # should return <blank>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_-aluEdaDTO",
    "outputId": "1d505c78-8b7a-4f34-f3b2-8e10f21eed93"
   },
   "outputs": [],
   "source": [
    "# Verify that Miniconda installation and updation worked\n",
    "\n",
    "!which conda # should return /usr/local/bin/conda\n",
    "!conda --version # should return 4.10.3\n",
    "!which python # should return /usr/local/bin/python\n",
    "!python --version # should return Python 3.6.13 :: Anaconda, Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmrAJyYMabHA",
    "outputId": "2c4fa3ed-9a53-44b7-acf9-162c176acd76"
   },
   "outputs": [],
   "source": [
    "# Overview of path files\n",
    "\n",
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XA9sYrV4akGw",
    "outputId": "01d98d3f-d4e8-453d-d322-8d51dfedad5d"
   },
   "outputs": [],
   "source": [
    "# View different installed packages\n",
    "\n",
    "!ls /usr/local/lib/python3.7/dist-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0Ppwdu0anYJ"
   },
   "outputs": [],
   "source": [
    "# Appending to sys path. This is where your installs will be located\n",
    "\n",
    "import sys\n",
    "_ = (sys.path\n",
    "        .append(\"/usr/local/lib/python3.7/site-packages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0cl_SMOa2AN",
    "outputId": "33bc492c-1f35-4422-c8b4-d96cbbf50b4f"
   },
   "outputs": [],
   "source": [
    "# To install any packages, run a command similar to the one below, pip also works\n",
    "!conda install --channel conda-forge featuretools --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhFA-xKkbHTd"
   },
   "source": [
    "### Xtreme codebase setup\n",
    "\n",
    "Now, we will set up the Xtreme repo ([link](https://github.com/google-research/xtreme)). The below cells do the following:\n",
    "1. Clone Xtreme\n",
    "2. Create a Conda env called ```xtreme``` and install dependencies into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFxzWzJga9hk",
    "outputId": "3049c581-b104-4934-b909-901e515b4d86"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd drive/MyDrive/ # Optional but recommended as we will be modifying Xtreme code below\n",
    "git clone https://github.com/google-research/xtreme.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4Bu1kblBWy8"
   },
   "source": [
    "This cell below is a modified version of ```xtreme/install_tools.sh```. \n",
    "\n",
    "**Note:** who are using Xtreme repo locally may also encounter errors with the original script, such as with ```conda activate```. You can copy-paste this script to resolve the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsj_ukAgbYIs",
    "outputId": "666d8b19-4b86-4bf7-f294-606ed58018e7"
   },
   "outputs": [],
   "source": [
    "# First, we need to install required dependencies. Instead of running their install_tools.sh, run this cell, which has a few minor modifications. This may take a few minutes to run.\n",
    "\n",
    "%%bash\n",
    "cd drive/MyDrive/ # Optional but recommended\n",
    "cd xtreme/\n",
    "# Copyright 2020 Google and DeepMind.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "set +eux  # for easier debugging\n",
    "\n",
    "REPO=$PWD\n",
    "LIB=$REPO/third_party\n",
    "mkdir -p $LIB\n",
    "\n",
    "# install conda env\n",
    "conda create --yes --name xtreme --file conda-env.txt \n",
    "conda init bash\n",
    "source activate xtreme\n",
    "set -eux\n",
    "\n",
    "# install latest transformer\n",
    "cd $LIB\n",
    "rm -rf transformers\n",
    "git clone https://github.com/huggingface/transformers\n",
    "cd transformers\n",
    "git checkout cefd51c50cc08be8146c1151544495968ce8f2ad --force\n",
    "pip install .\n",
    "cd $LIB\n",
    "\n",
    "pip install seqeval\n",
    "pip install tensorboardx\n",
    "pip install tqdm\n",
    "\n",
    "# install XLM tokenizer\n",
    "pip install sacremoses\n",
    "pip install pythainlp\n",
    "pip install jieba\n",
    "\n",
    "#git clone https://github.com/neubig/kytea.git && cd kytea\n",
    "#./configure --prefix=${CONDA_PREFIX}\n",
    "#make && make install\n",
    "pip install kytea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4DVHoBlCuAa"
   },
   "source": [
    "## Training\n",
    "\n",
    "Although this codebase can be used for many varieties of training methods and experiments, we will only train a straightforward baseline. We will create a monolingual Hindi QA model. We encourage you to read and experiment with the Xtreme codebase, and also with other repos. Some promising avenues:\n",
    "\n",
    "* Train model on both Hindi and Tamil ChAII data,\n",
    "* Multi-task learning with Xtreme,\n",
    "* Annotate your own data into a QA format and augment training,\n",
    "* Zero-shot transfer learning\n",
    "\n",
    "The cells below do the following:\n",
    "\n",
    "1. Convert the given ChAII data (from competition) to QA (SQuAD) format, split into train and dev sets.\n",
    "2. Finetune mBERT (bert-base-multilingual-cased) on the ChAII data.\n",
    "3. Save the model and dev predictions into GDrive for evaluation later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwbfBnSXDcQk"
   },
   "source": [
    "### Data conversion to QA format\n",
    "\n",
    "The below cell converts the ChAII data from the TyDiQA format to the SQuAD QA format, so it can be used with the Xtreme pipeline. You need to download the ChAII data from Kaggle, and either upload on your Google Drive or locally onto the Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "aE9wX7A8D-me",
    "outputId": "38ff7707-6fd1-4eb7-88d9-211643eb2df7"
   },
   "outputs": [],
   "source": [
    "# Convert TyDiQA format to a QA format\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"/root/mount/dataset\"\n",
    "json_dicts = []\n",
    "\n",
    "with open(os.path.join(data_path,\"chaii_tydiqa_answer_labeling_hi_train.jsonl\"),'r') as f:\n",
    "  for line in tqdm(f):\n",
    "    json_dict = json.loads(line)\n",
    "    json_dicts.append(json_dict)\n",
    "\n",
    "print(\"TyDiQA format:\")\n",
    "pprint(json_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf6zv9YfJD5D"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of datapoints: %d\" % len(json_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN5l8MNZJHy1"
   },
   "outputs": [],
   "source": [
    "# Split datapoints language-wise and into QA format\n",
    "# Run this cell only if you need to convert from TyDiQA to SQuAD format, otherwise run the nexy one.\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def byte_str(text):\n",
    "  return text.encode(\"utf-8\")\n",
    "\n",
    "def byte_len(text):\n",
    "  # Python 3 encodes text as character sequences, not byte sequences\n",
    "  # (like Python 2).\n",
    "  return len(byte_str(text))\n",
    "\n",
    "def byte_slice(text, start, end, errors=\"replace\"):\n",
    "  # Python 3 encodes text as character sequences, not byte sequences\n",
    "  # (like Python 2).\n",
    "  return byte_str(text)[start:end].decode(\"utf-8\", errors=errors)\n",
    "\n",
    "def convert_to_qa_format(tydi_json):\n",
    "  answer = {}\n",
    "  for annotation in tydi_json[\"annotations\"]:\n",
    "    minimal_answer = annotation[\"minimal_answer\"]\n",
    "    if minimal_answer[\"plaintext_start_byte\"] != -1 and minimal_answer[\"plaintext_end_byte\"] != -1:\n",
    "      answer[\"text\"] = byte_slice(tydi_json[\"document_plaintext\"],minimal_answer[\"plaintext_start_byte\"],minimal_answer[\"plaintext_end_byte\"])\n",
    "      answer[\"answer_start\"] = [m.start() for m in re.finditer(answer[\"text\"],tydi_json[\"document_plaintext\"])][0]\n",
    "      break\n",
    "  if answer == {}:\n",
    "    return {}\n",
    "  \n",
    "  qa_json = {\n",
    "      \"title\" : tydi_json[\"document_title\"],\n",
    "      \"paragraphs\" : [\n",
    "                      {\n",
    "                          \"context\": tydi_json[\"document_plaintext\"],\n",
    "                          \"qas\" : [\n",
    "                                   {\n",
    "                                    \"question\" : tydi_json[\"question_text\"],\n",
    "                                    \"id\" : tydi_json[\"language\"] + '-' + str(tydi_json[\"example_id\"]),\n",
    "                                    \"answers\" : [answer],\n",
    "                                   }\n",
    "                          ]\n",
    "                      }\n",
    "      ],\n",
    "  }\n",
    "\n",
    "  return qa_json\n",
    "\n",
    "  \n",
    "language_list = [\n",
    "       'tamil',\n",
    "  ]\n",
    "\n",
    "\n",
    "qa_data = {\"data\":[], \"version\":f\"TyDiQA_chaii_hi\"}\n",
    "for json_dict in json_dicts:\n",
    "  if json_dict[\"language\"] in language_list:\n",
    "    qa_datapoint = convert_to_qa_format(json_dict)\n",
    "    if qa_datapoint != {}:\n",
    "      qa_data[\"data\"].append(qa_datapoint)\n",
    "    qa_data['data'].append(json_dict)\n",
    "\n",
    "print(\"QA (SQuAD) format:\")\n",
    "print(qa_data[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you skipped the previous cell\n",
    "qa_data = {\"data\":[], \"version\":f\"TyDiQA_chaii_hi\"}\n",
    "for json_dict in json_dicts:\n",
    "    qa_data['data'].append(json_dict)\n",
    "print(\"QA (SQuAD) format:\")\n",
    "print(qa_data[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5se7aK7JoGM"
   },
   "outputs": [],
   "source": [
    "# Splitting data into train and dev and saving converted QA formats\n",
    "\n",
    "import random\n",
    "\n",
    "qa_data_datapoints = qa_data[\"data\"]\n",
    "random.shuffle(qa_data_datapoints)\n",
    "train_size = int(len(qa_data_datapoints)*0.8)\n",
    "train_qa_data_datapoints, dev_qa_data_datapoints = qa_data_datapoints[:train_size], qa_data_datapoints[train_size:]\n",
    "\n",
    "train_qa_data = {\"data\":train_qa_data_datapoints, \"version\":f\"TyDiQA_chaii_hi_train\"}\n",
    "dev_qa_data = {\"data\":dev_qa_data_datapoints, \"version\":f\"TyDiQA_chaii_hi_dev\"}\n",
    "\n",
    "with open(os.path.join(data_path,\"train.hi.qa.jsonl\"),'w') as f:\n",
    "  json.dump(train_qa_data,f)\n",
    "\n",
    "with open(os.path.join(data_path,\"dev.hi.qa.jsonl\"),'w') as f:\n",
    "  json.dump(dev_qa_data,f)\n",
    "\n",
    "print(\"Training data size: %d\" % len(train_qa_data_datapoints))\n",
    "print(\"Dev data size: %d\" % len(dev_qa_data_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scuzryHrPzHV"
   },
   "source": [
    "The cell below is optional (we have not used it for our baseline model), but it downloads the original TyDiQA data in the QA format. You can combine it with our ChAII data and boost training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta0K5sV8b6nk"
   },
   "outputs": [],
   "source": [
    "# Downloading the data. For baseline, we can ignore the other training datasets for other tasks, and focus on training with just TyDiQA data. \n",
    "\n",
    "%%bash\n",
    "source activate xtreme\n",
    "cd drive/MyDrive/ # Optional but recommended\n",
    "cd xtreme/\n",
    "# Copyright 2020 Google and DeepMind.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "REPO=$PWD\n",
    "DIR=$REPO/download/\n",
    "mkdir -p $DIR\n",
    "\n",
    "function download_tydiqa {\n",
    "    echo \"download tydiqa-goldp\"\n",
    "    base_dir=$DIR/tydiqa/\n",
    "    mkdir -p $base_dir && cd $base_dir\n",
    "    tydiqa_train_file=tydiqa-goldp-v1.1-train.json\n",
    "    tydiqa_dev_file=tydiqa-goldp-v1.1-dev.tgz\n",
    "    wget https://storage.googleapis.com/tydiqa/v1.1/${tydiqa_train_file} -q --show-progress\n",
    "    wget https://storage.googleapis.com/tydiqa/v1.1/${tydiqa_dev_file} -q --show-progress\n",
    "    tar -xf ${tydiqa_dev_file}\n",
    "    rm ${tydiqa_dev_file}\n",
    "    out_dir=$base_dir/tydiqa-goldp-v1.1-train\n",
    "    python $REPO/utils_preprocess.py --data_dir $base_dir --output_dir $out_dir --task tydiqa\n",
    "    mv $base_dir/$tydiqa_train_file $out_dir/\n",
    "    echo \"Successfully downloaded data at $DIR/tydiqa\" >> $DIR/download.log\n",
    "}\n",
    "\n",
    "download_tydiqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX8uTFPfQQJR"
   },
   "source": [
    "### Training mBERT on Hindi ChAII data\n",
    "\n",
    "The below script uses the Xtreme script to train the data. Here, we need to modify the code in the folders to train it on the ChAII data. You can double click on the scripts, modify the code and change them. \n",
    "\n",
    "For the baseline, the following changes were made to the Xtreme repo code:\n",
    "\n",
    "\n",
    "1.   In ```scripts/train.sh```, an additional task called \"chaii_hi\" was added as such:\n",
    "```\n",
    "...\n",
    "elif [ $TASK == 'chaii_hi' ]; then\n",
    "  bash $REPO/scripts/train_qa.sh $MODEL chaii_hi $TASK $GPU $DATA_DIR $OUT_DIR\n",
    "...\n",
    "```\n",
    "2.   In ```scripts/train_qa.sh```, the following flags were added:\n",
    "```\n",
    "TRAIN_LANG=\"en\"\n",
    "EVAL_LANG=\"en\"\n",
    "```\n",
    "Another elif condition was added as such to modify path of data dir:\n",
    "```\n",
    "...\n",
    "elif [ $SRC == 'chaii_hi' ]; then\n",
    "  TASK_DATA_DIR=\"/content/drive/MyDrive/chaii_data\"\n",
    "  TRAIN_FILE=${TASK_DATA_DIR}/train.hi.qa.jsonl\n",
    "  PREDICT_FILE=${TASK_DATA_DIR}/dev.hi.qa.jsonl\n",
    "  TRAIN_LANG=\"hi\"\n",
    "  EVAL_LANG=\"hi\"\n",
    "...\n",
    "```\n",
    "Finally, TRAIN_LANG and EVAL_LANG replaced the hardcoded \"en\":\n",
    "```\n",
    " --weight_decay 0.0001 \\\n",
    "  --threads 8 \\\n",
    "  --train_lang ${TRAIN_LANG} \\\n",
    "  --eval_lang ${EVAL_LANG}\n",
    "```\n",
    "\n",
    "Since you may be making your own changes for experimentation, it is HIGHLY RECOMMENDED to clone the Xtreme repo into your GDrive.\n",
    "\n",
    "Finally, we create a run.sh script in the current root directory (```/content```), and paste the following commands:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "source activate xtreme\n",
    "cd drive/MyDrive/ # Optional but recommended\n",
    "cd xtreme\n",
    "bash scripts/train.sh bert-base-multilingual-cased chaii_hi\n",
    "```\n",
    "\n",
    "Your model should be stored in ```xtreme/outputs-temp/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww3D25ezeMIT"
   },
   "outputs": [],
   "source": [
    "# Now that the data is downloaded, you can run the training script directly from the repo. Here the best way to do it, is to create a new file called run.sh in home folder, and copy paste the below commands, then just run this cell:\n",
    "# Also ensure that you set your runtime type to GPU for training.\n",
    "'''\n",
    "#!/bin/bash\n",
    "\n",
    "source activate xtreme\n",
    "cd drive/MyDrive/\n",
    "cd xtreme\n",
    "bash scripts/train.sh bert-base-multilingual-cased chaii_hi\n",
    "'''\n",
    "\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime\n",
    "\n",
    "!bash /root/run.sh chaii_hi \"/root/mount/dataset\" \"/root/mount/outputs-temp\" bert-base-multilingual-cased 0 train.hi.qa.jsonl dev.hi.qa.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmVxDvoTuew6"
   },
   "source": [
    "## Inference and Evaluation\n",
    "\n",
    "For inference, we do the following modifications to Xtreme repo:\n",
    "1. In ```predict_qa.sh```, add the following (line 40):\n",
    "```\n",
    "elif [ $TGT == 'chaii_hi' ]; then\n",
    "  langs=( hi )\n",
    "```\n",
    "\n",
    "\n",
    "Also, we create a bash file (similar to ```run.sh```) called ```predict.sh```, and copy the commands below into it:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "source activate xtreme\n",
    "cd drive/MyDrive/\n",
    "cd xtreme\n",
    "\n",
    "MODEL_PATH=\"/content/drive/MyDrive/xtreme/outputs-temp/chaii_hi/bert-base-multilingual-cased_LR3e-5_EPOCH2.0_maxlen384\"\n",
    "GPU=-0\n",
    "DATA_DIR=\"/content/drive/MyDrive/\"\n",
    "\n",
    "bash scripts/predict_qa.sh bert-base-multilingual-cased bert chaii_hi $MODEL_PATH chaii_hi $GPU $DATA_DIR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNbJn6KoNIMg"
   },
   "outputs": [],
   "source": [
    "# First, you need to run inference on the models\n",
    "\n",
    "!bash /root/predict.sh \"/root/mount/outputs-temp/chaii_hi/bert-base-multilingual-cased_LR3e-5_EPOCH2.0_maxlen384\" chaii_hi \"/root/mount/dataset\" \"/root/eval_dir/predictions\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlPo9R0Duc3U"
   },
   "outputs": [],
   "source": [
    "# Before you can start local evaluation, we need to put files in a particular format. Run the below command to transfer predictions and labels\n",
    "# Copy dev data\n",
    "%%bash\n",
    "\n",
    "# cd drive/MyDrive # Optional but recommended\n",
    "\n",
    "TASK_NAME=\"chaii_hi\"\n",
    "mkdir -p /root/eval_dir/predictions/$TASK_NAME/\n",
    "mkdir -p /root/eval_dir/labels/$TASK_NAME/\n",
    "\n",
    "GOLD_DATA_LOCATION=\"/root/mount/dataset\"\n",
    "\n",
    "cp $GOLD_DATA_LOCATION/* eval_dir/labels/$TASK_NAME/\n",
    "for file in eval_dir/labels/$TASK_NAME/dev.*.jsonl; do\n",
    "filename=$(basename \"$file\")\n",
    "fname=\"${filename%.*}\"\n",
    "lg=$(echo $fname | cut -d\".\" -f 2)\n",
    "mv $file eval_dir/labels/$TASK_NAME/test-$lg.json\n",
    "done\n",
    "\n",
    "cp xtreme/predictions/$TASK_NAME/predictions* eval_dir/predictions/$TASK_NAME/\n",
    "for file in eval_dir/predictions/$TASK_NAME/predictions*.json; do\n",
    "filename=$(basename \"$file\")\n",
    "fname=\"${filename%.*}\"\n",
    "lg=$(echo $fname | cut -d\"_\" -f 2)\n",
    "mv $file eval_dir/predictions/tydiqa/test-$lg.json\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Mkrngi9hlJQ"
   },
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "%%bash\n",
    "\n",
    "source activate xtreme\n",
    "cd xtreme/\n",
    "python evaluate.py --prediction_folder /root/eval_dir/predictions --label_folder /root/eval_dir/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP4_O2spMxYW"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/root/mount/outputs-temp/chaii_hi/bert-base-multilingual-cased_LR3e-5_EPOCH2.0_maxlen384/predictions_hi_.json\") as f:\n",
    "  preds = json.load(f)\n",
    "\n",
    "with open(\"/root/mount/dataset/dev.hi.qa.jsonl\") as f:\n",
    "  dev_data = json.load(f)\n",
    "\n",
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qugMjeD1d1_m"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "dev_answer_pair_matches = []\n",
    "for d in dev_data['data']:\n",
    "  for para in d['paragraphs']:\n",
    "    for qa in para['qas']:\n",
    "      dev_answer_pair_matches.append({'context':para['context'],'question':qa['question'],'gold_answer':qa['answers'],'mbert_pred':preds[qa['id']],'id':qa['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72LNd7Mvedgv"
   },
   "outputs": [],
   "source": [
    "pprint(dev_answer_pair_matches[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spOJDJpZgHmF"
   },
   "outputs": [],
   "source": [
    "#Matches in predictions\n",
    "correct_ans = [d for d in dev_answer_pair_matches if d['mbert_pred']==d['gold_answer'][0]['text']]\n",
    "with open('/root/mount/correct_chaii_hi_mbert.txt','w',encoding='utf-8') as f:\n",
    "  for c in correct_ans:\n",
    "    f.write(f\"id:{c['id']}\\n\")\n",
    "    f.write(f\"context:{c['context']}\\n\")\n",
    "    f.write(f\"question:{c['question']}\\n\")\n",
    "    f.write(f\"gold_answer:{c['gold_answer'][0]['text']}\\n\")\n",
    "    f.write(f\"mbert_pred:{c['mbert_pred']}\\n\")\n",
    "    f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JHfrNyPhrJW"
   },
   "outputs": [],
   "source": [
    "#Mismatches in predictions\n",
    "wrong_ans = [d for d in dev_answer_pair_matches if d['mbert_pred']!=d['gold_answer'][0]['text']]\n",
    "with open('/root/mount/wrong_chaii_hi_mbert.txt','w',encoding='utf-8') as f:\n",
    "  for c in wrong_ans:\n",
    "    f.write(f\"id:{c['id']}\\n\")\n",
    "    f.write(f\"context:{c['context']}\\n\")\n",
    "    f.write(f\"question:{c['question']}\\n\")\n",
    "    f.write(f\"gold_answer:{c['gold_answer'][0]['text']}\\n\")\n",
    "    f.write(f\"mbert_pred:{c['mbert_pred']}\\n\")\n",
    "    f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84li0T-PlJdi"
   },
   "outputs": [],
   "source": [
    "len(correct_ans),len(wrong_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-FWJFCol_Du"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ChAII_COPY.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
